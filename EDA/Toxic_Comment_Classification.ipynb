{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fac8c8",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Toxic Comment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606fdc9",
   "metadata": {},
   "source": [
    "This Jupyter Notebook outlines the approach to build a multi-headed model for the Kaggle competition, which involves predicting different types of comment toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac57bd4",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8361db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132d580",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9ddb8",
   "metadata": {},
   "source": [
    "# Distribution of Toxicity Classes\n",
    "\n",
    "The following bar chart shows the distribution of different types of toxicity in the dataset. This visualization helps in understanding the prevalence of each type of toxicity within the comments. Such insights are crucial for tailoring the model to better recognize and differentiate between these types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting occurrences for each type of toxicity\n",
    "counts = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum()\n",
    "\n",
    "# Creating a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts.plot(kind='bar')\n",
    "plt.title('Distribution of Toxicity Classes')  # Title of the plot\n",
    "plt.xlabel('Toxicity Type')                    # Label for the X-axis\n",
    "plt.ylabel('Number of Occurrences')            # Label for the Y-axis\n",
    "plt.xticks(rotation=45)                        # Rotate labels on X-axis for better readability\n",
    "plt.show()                                     # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f12bd",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords and wordnet from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'\\\\W+', ' ', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean the text data\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f675f4f",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39201b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a5c6a",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_full = train_data[labels]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(X_train_tfidf, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = OneVsRestClassifier(LogisticRegression(solver='liblinear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adb808",
   "metadata": {},
   "source": [
    "## Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb45890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict_proba(X_val)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc_scores = [roc_auc_score(y_val[label], y_pred[:, i]) for i, label in enumerate(labels)]\n",
    "mean_roc_auc = np.mean(roc_auc_scores)\n",
    "print('Mean Column-wise ROC AUC:', mean_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875a48e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf4a0f",
   "metadata": {},
   "source": [
    "Hyperparameter tuning involves adjusting the parameters of the machine learning model to improve its performance. For Logistic Regression, a key hyperparameter to tune is the regularization strength (C). Regularization can help prevent overfitting. A lower value of C specifies stronger regularization.\n",
    "\n",
    "We will use Grid Search technique for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {'estimator__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train_split)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497a189",
   "metadata": {},
   "source": [
    "## Final Model Training and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990f2b5",
   "metadata": {},
   "source": [
    "Train the model on the entire dataset and make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train_tfidf, y_full)\n",
    "\n",
    "# Preprocess the test data\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(clean_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['comment_text'])\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = best_model.predict_proba(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a95e3",
   "metadata": {},
   "source": [
    "## Preparing Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cad01e",
   "metadata": {},
   "source": [
    "Format the predictions as required for the competition submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission dataframe\n",
    "submission = pd.DataFrame(test_predictions, columns=labels)\n",
    "submission['id'] = test_data['id']\n",
    "submission = submission[['id'] + labels]\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
