{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fac8c8",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Toxic Comment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606fdc9",
   "metadata": {},
   "source": [
    "This Jupyter Notebook outlines the approach to build a multi-headed model for the Kaggle competition, which involves predicting different types of comment toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac57bd4",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8361db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud,  STOPWORDS\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132d580",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../src/data/train.csv'\n",
    "test_path = '../src/data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9ddb8",
   "metadata": {},
   "source": [
    "## Distribution of Toxicity Classes\n",
    "\n",
    "The following bar chart shows the distribution of different types of toxicity in the dataset. This visualization helps in understanding the prevalence of each type of toxicity within the comments. Such insights are crucial for tailoring the model to better recognize and differentiate between these types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting occurrences for each type of toxicity\n",
    "counts = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum()\n",
    "\n",
    "# Creating a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts.plot(kind='bar')\n",
    "plt.title('Distribution of Toxicity Classes')  # Title of the plot\n",
    "plt.xlabel('Toxicity Type')                    # Label for the X-axis\n",
    "plt.ylabel('Number of Occurrences')            # Label for the Y-axis\n",
    "plt.xticks(rotation=45)                        # Rotate labels on X-axis for better readability\n",
    "plt.show()                                     # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce58ce",
   "metadata": {},
   "source": [
    "## Words and Phrases Commonly Found in Each Class of Toxic Comments\n",
    "\n",
    "The word cloud below visualizes the most frequent words found in each class of toxic comments. This visualization helps in identifying key themes and terms that are commonly associated with toxicity in the dataset. Such insights can be useful for understanding the nature of the toxic content and for further refining the model to better recognize toxic comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ed6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "  # Combine all toxic comments into a single text string\n",
    "  toxic_comments = train_data[train_data[label] == 1]['comment_text'].str.cat(sep=' ')\n",
    "\n",
    "  # Create a word cloud\n",
    "  wordcloud = WordCloud(width = 800, height = 800, \n",
    "                  background_color ='white', \n",
    "                  stopwords = set(STOPWORDS), \n",
    "                  min_font_size = 10).generate(toxic_comments)\n",
    "\n",
    "  # Visualize the word cloud\n",
    "  plt.figure(figsize = (8, 8), facecolor = None) \n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.title(f'Word Cloud for {label}')\n",
    "  plt.axis(\"off\") \n",
    "  plt.tight_layout(pad = 0) \n",
    "    \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663d8d8",
   "metadata": {},
   "source": [
    "## Correlation Between Different Types of Toxicity\n",
    "\n",
    "The heatmap below shows the correlations between different types of toxicity in the dataset. This visualization is crucial to understand how these types of toxicity are related. For example, a high correlation between two types would suggest that they often occur together in the comments. Understanding these relationships can be useful for feature engineering and improving model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].corr()\n",
    "\n",
    "# Create a heatmap for visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Between Different Types of Toxicity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f12bd",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8214eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments Length Before Cleaning\n",
    "train_data['original_len'] = train_data['comment_text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords and wordnet from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'\\\\W+', ' ', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean the text data\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a248425",
   "metadata": {},
   "source": [
    "## Visualization of the Distribution of the Length of Comments\n",
    "Visualization of the distribution of the length of comments before and after cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments Length After cleaning\n",
    "train_data['cleaned_len'] = train_data['comment_text'].apply(clean_text).apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Use a larger bin size for better visibility and adjust alpha for transparency\n",
    "plt.hist(train_data['original_len'], bins=30, alpha=0.5, label='Original Length')\n",
    "plt.hist(train_data['cleaned_len'], bins=30, alpha=0.5, label='Cleaned Length')\n",
    "\n",
    "plt.title('Distribution of Comment Lengths Before and After Cleaning')\n",
    "plt.xlabel('Length of comments')\n",
    "plt.ylabel('Number of comments')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f675f4f",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39201b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43028a50",
   "metadata": {},
   "source": [
    "## Visualization of TF-IDF distribution\n",
    "Visualization of the distribution of the most important words (according to TF-IDF) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20147c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_scores = np.array(X_train_tfidf.sum(axis=0)).ravel()\n",
    "tfidf_ranking = np.argsort(tfidf_scores)[::-1]\n",
    "top_n = 20  # Top 20 words\n",
    "top_features = [tfidf_vectorizer.get_feature_names_out()[i] for i in tfidf_ranking[:top_n]]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_features, y=tfidf_scores[tfidf_ranking[:top_n]])\n",
    "plt.title('Top TF-IDF Scores in the Corpus')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a5c6a",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_full = train_data[labels]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(X_train_tfidf, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = OneVsRestClassifier(LogisticRegression(solver='liblinear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adb808",
   "metadata": {},
   "source": [
    "## Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb45890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict_proba(X_val)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc_scores = [roc_auc_score(y_val[label], y_pred[:, i]) for i, label in enumerate(labels)]\n",
    "mean_roc_auc = np.mean(roc_auc_scores)\n",
    "print('Mean Column-wise ROC AUC:', mean_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875a48e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf4a0f",
   "metadata": {},
   "source": [
    "Hyperparameter tuning involves adjusting the parameters of the machine learning model to improve its performance. For Logistic Regression, a key hyperparameter to tune is the regularization strength (C). Regularization can help prevent overfitting. A lower value of C specifies stronger regularization.\n",
    "\n",
    "We will use Grid Search technique for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {'estimator__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train_split)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497a189",
   "metadata": {},
   "source": [
    "## Final Model Training and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990f2b5",
   "metadata": {},
   "source": [
    "Train the model on the entire dataset and make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train_tfidf, y_full)\n",
    "\n",
    "# Preprocess the test data\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(clean_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['comment_text'])\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = best_model.predict_proba(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a95e3",
   "metadata": {},
   "source": [
    "## Preparing Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cad01e",
   "metadata": {},
   "source": [
    "Format the predictions as required for the competition submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission dataframe\n",
    "submission = pd.DataFrame(test_predictions, columns=labels)\n",
    "submission['id'] = test_data['id']\n",
    "submission = submission[['id'] + labels]\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('../src/data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
