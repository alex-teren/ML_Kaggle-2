{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9f25e3",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - Dimensionality Reduction and Clustering\n",
    "\n",
    "## Objective\n",
    "The goal of this homework assignment is to gain practical experience in performing unsupervised learning tasks, including dimensionality reduction and clustering, using a real dataset.\n",
    "\n",
    "## Dataset Description\n",
    "The dataset consists of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "## Files\n",
    "- train.csv: Training set, contains comments with their binary labels\n",
    "- test.csv: Test set, where we predict the toxicity probabilities\n",
    "- sample_submission.csv: Sample submission file in the correct format\n",
    "\n",
    "We will perform data exploration, preprocessing, dimensionality reduction, clustering, and visualize the results.\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc427717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af4129",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing\n",
    "\n",
    "First, we'll load the data, explore its characteristics, and preprocess it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1532a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (adjust the file paths as needed)\n",
    "train_df = pd.read_csv('train.csv').sample(n=10000, random_state=42) # Selecting a random subset of 10,000 samples\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bbe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps including text cleaning and vectorization\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing special characters and digits\n",
    "    text_clean = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Converting to lower case\n",
    "    text_clean = text_clean.lower()\n",
    "    return text_clean\n",
    "\n",
    "# Apply cleaning function to the comment text\n",
    "train_df['cleaned_text'] = train_df['comment_text'].apply(clean_text)\n",
    "\n",
    "# Vectorizing the text using TfIdfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
    "tfidf_vectors = vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "\n",
    "print(\"Shape of the TfIdf matrix:\", tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde0332",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "We will apply PCA and UMAP to reduce the dimensionality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762085d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA for visualization: Reducing to 2 components\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d_result = pca_2d.fit_transform(tfidf_vectors.toarray())\n",
    "\n",
    "# Display the explained variance for 2 components\n",
    "print(\"Explained variance (2 components):\", pca_2d.explained_variance_ratio_.sum())\n",
    "\n",
    "# Optionally, also reduce to 3 components for a different visualization\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d_result = pca_3d.fit_transform(tfidf_vectors.toarray())\n",
    "\n",
    "# Display the explained variance for 3 components\n",
    "print(\"Explained variance (3 components):\", pca_3d.explained_variance_ratio_.sum())\n",
    "\n",
    "# Visualization of the PCA result (2D)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_2d_result[:, 0], pca_2d_result[:, 1])\n",
    "plt.title('PCA - 2 Components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34031eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Applying UMAP to reduce the dataset to 2 components for visualization\n",
    "umap_2d = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_2d_result = umap_2d.fit_transform(tfidf_vectors.toarray())\n",
    "\n",
    "# Visualization of the 2D UMAP result\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(umap_2d_result[:, 0], umap_2d_result[:, 1])\n",
    "plt.title('UMAP - 2 Components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optionally, also reduce to 3 components for a different perspective\n",
    "umap_3d = umap.UMAP(n_components=3, random_state=42)\n",
    "umap_3d_result = umap_3d.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded5c88",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We will perform clustering using K-Means, DBSCAN, and Hierarchical Clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Elbow Method to find the optimal number of clusters\n",
    "inertia = []\n",
    "k_range = range(1, 11)  # Trying different numbers of clusters\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(tfidf_vectors.toarray())\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Determining Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose the number of clusters where the elbow occurs\n",
    "optimal_k = 4  # Replace with the appropriate number as per the elbow plot\n",
    "\n",
    "# Performing K-Means clustering with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_clusters = kmeans.fit_predict(tfidf_vectors.toarray())\n",
    "\n",
    "# The variable 'kmeans_clusters' now holds the cluster assignments for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying DBSCAN clustering\n",
    "# Note: The parameters 'eps' and 'min_samples' may need to be adjusted based on your dataset\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_clusters = dbscan.fit_predict(tfidf_vectors.toarray())\n",
    "\n",
    "# dbscan_clusters now contains the cluster labels for each data point\n",
    "# Noise points are labeled as -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Hierarchical Clustering\n",
    "# 'optimal_k' should be the number of clusters you determined to be optimal (e.g., from the Elbow Method in K-Means)\n",
    "optimal_k = 4  # Replace with your chosen number of clusters\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "hierarchical_clusters = hierarchical.fit_predict(tfidf_vectors.toarray())  # Replace with your vectorized data\n",
    "\n",
    "# hierarchical_clusters now contains the cluster assignments for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812f807",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Finally, we will visualize the clusters created by each clustering method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a725c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for K-Means Clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_2d_result[:, 0], pca_2d_result[:, 1], c=kmeans_clusters)\n",
    "plt.title('K-Means Clustering Visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "# Visualization for DBSCAN Clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_2d_result[:, 0], pca_2d_result[:, 1], c=dbscan_clusters)\n",
    "plt.title('DBSCAN Clustering Visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "# Visualization for Hierarchical Clustering\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_2d_result[:, 0], pca_2d_result[:, 1], c=hierarchical_clusters)\n",
    "plt.title('Hierarchical Clustering Visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
